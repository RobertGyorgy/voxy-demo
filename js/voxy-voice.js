/* ================================
   VOXY VOICE AI INTEGRATION
   ================================
   Integrates OpenAI Realtime API for voice conversations
   Based on realtime-voice-ai backend
*/

class VoxyVoice {
  constructor(apiKey, voice = 'ballad') {
    console.log('üé§ VoxyVoice constructor called with voice:', voice);
    
    this.apiKey = apiKey;
    this.voice = voice;
    this.ws = null;
    this.isConnected = false;
    this.isListening = false;
    
    // Audio contexts
    this.audioContext = null;
    this.mediaStream = null;
    this.audioProcessor = null;
    this.nextPlayTime = 0;
    this.currentAudioSources = [];
    
    // Rate limiting for audio chunks
    this.lastAudioChunkTime = 0;
    this.audioChunkInterval = 100; // Minimum 100ms between audio chunks (10 chunks/second max)
    this.isProcessingAudio = false;
    
    // Time limit for conversation (1 minute from first AI response)
    this.conversationStartTime = null;
    this.conversationTimeLimit = 60000; // 1 minute in milliseconds
    this.timeLimitTimer = null;
    
    // Configuration from realtime-voice-ai/config.js
    this.config = {
      REALTIME_API_URL: 'wss://api.openai.com/v1/realtime',
      MODEL: 'gpt-realtime-mini',
      AUDIO: {
        sampleRate: 24000,
        playbackSpeed: 0.95
      },
      SESSION: {
        turn_detection: {
          type: 'server_vad',
          threshold: 0.5,
          prefix_padding_ms: 300,
          silence_duration_ms: 2000 // Increased to reduce false triggers
        },
        temperature: 0.8,
        instructions: `Tu e»ôti Voxy ‚Äî un agent vocal conversa»õional de ultimƒÉ genera»õie, dezvoltat manual de o echipƒÉ ambi»õioasƒÉ din Bra»ôov, Rom√¢nia.
E»ôti creatƒÉ pentru a vorbi natural, fluid »ôi inteligent, √Æn peste 40 de limbi.

PRIMA INTERAC»öIUNE - SALUTUL TƒÇU INI»öIAL:
C√¢nd e»ôti activatƒÉ pentru prima datƒÉ, te prezin»õi astfel:
"Salut! Eu sunt Voxy, agentul vocal AI creat de o echipƒÉ din Bra»ôov. Pot vorbi √Æn peste 40 de limbi »ôi ajut companiile sƒÉ √ÆnlocuiascƒÉ operatorii de call center. Cu ce te pot ajuta astƒÉzi?"

DupƒÉ salutul ini»õial, conduci conversa»õia natural:
- Ascul»õi activ »ôi rƒÉspunzi relevant
- Pui √ÆntrebƒÉri clare despre nevoile utilizatorului
- Explici func»õionalitƒÉ»õile VOXY c√¢nd e»ôti √ÆntrebatƒÉ
- Men»õii un ton profesionist dar prietenos
- RƒÉspunzi concis »ôi clar

FUNC»öIONALITƒÇ»öI VOXY (pentru c√¢nd e»ôti √ÆntrebatƒÉ):
1. RƒÉspunde la apeluri telefonice 24/7
2. Face apeluri de v√¢nzƒÉri automate
3. Se integreazƒÉ cu CRM-uri
4. Vorbe»ôte √Æn 40+ limbi
5. AnalizƒÉ »ôi raportare automatƒÉ

Vorbe»ôte natural, fƒÉrƒÉ jargon tehnic, »ôi adapteazƒÉ-te la √ÆntrebƒÉrile utilizatorului.`
      }
    };
  }
  
  // Connect to OpenAI Realtime API
  async connect() {
    console.log('üîå Connecting to OpenAI Realtime API...');
    
    // Load API key if not already set
    if (!this.apiKey) {
      loadVoxyApiKey();
      if (!VOXY_CONFIG.apiKey) {
        throw new Error('No API key available. Please provide OpenAI API key.');
      }
      this.apiKey = VOXY_CONFIG.apiKey;
    }
    
    console.log('üîë Using API key:', this.apiKey.substring(0, 20) + '...');
    console.log('üîë Full API key length:', this.apiKey.length);
    console.log('üîë API key starts with:', this.apiKey.substring(0, 15));
    console.log('üîë API key ends with:', this.apiKey.substring(this.apiKey.length - 10));
    
    return new Promise(async (resolve, reject) => {
      try {
        // Use API key directly (injected by build.js from Vercel environment)
        const apiKeyToUse = this.apiKey;
        
        if (!apiKeyToUse) {
          throw new Error('No API key available. Please configure VOXY_API_KEY in Vercel environment variables.');
        }
        
        // Connect directly to OpenAI with the API key
        const url = `wss://api.openai.com/v1/realtime?model=${this.config.MODEL}`;
        console.log('üåê WebSocket URL:', url);
        
        const apiKeyHeader = `openai-insecure-api-key.${apiKeyToUse}`;
        console.log('üîë Using API key:', apiKeyToUse.substring(0, 20) + '...');
        
        this.ws = new WebSocket(url, [
          'realtime',
          apiKeyHeader,
          'openai-beta.realtime-v1'
        ]);
      } catch (error) {
        console.error('‚ùå Failed to get API key from proxy:', error);
        reject(error);
        return;
      }
      
      this.ws.binaryType = 'arraybuffer';
      
      this.ws.onopen = () => {
        console.log('‚úÖ WebSocket connected successfully');
        this.isConnected = true;
        this.sendSessionConfig();
        resolve();
      };
      
      this.ws.onmessage = (event) => {
        console.log('üì® WebSocket message received:', event.data);
        this.handleMessage(event);
      };
      
      this.ws.onerror = (error) => {
        console.error('‚ùå WebSocket error:', error);
        console.error('‚ùå Error details:', error.type, error.code);
        console.error('‚ùå WebSocket readyState:', this.ws.readyState);
        reject(error);
      };
      
      this.ws.onclose = (event) => {
        console.log('üîå WebSocket closed:', event.code, event.reason);
        this.isConnected = false;
        if (this.isListening) {
          this.stopListening();
        }
      };
      
      // Timeout after 5 seconds
      const timeoutId = setTimeout(() => {
        if (!this.isConnected) {
          console.error('‚è∞ Connection timeout after 5 seconds');
          console.error('‚è∞ WebSocket readyState:', this.ws.readyState);
          reject(new Error('Connection timeout after 5 seconds'));
        }
      }, 5000);
      
      // Clear timeout if connection succeeds
      this.ws.addEventListener('open', () => {
        clearTimeout(timeoutId);
      });
    });
  }
  
  // Send session configuration
  sendSessionConfig() {
    const config = {
      type: 'session.update',
      session: {
        modalities: ['text', 'audio'],
        instructions: this.config.SESSION.instructions,
        voice: this.voice,
        input_audio_format: 'pcm16',
        output_audio_format: 'pcm16',
        input_audio_transcription: {
          model: 'whisper-1-large' // Better accuracy for call center AI
        },
        // Force text processing to use cheaper model
        model: 'gpt-4o-mini', // Override default GPT-5 for text processing
        turn_detection: this.config.SESSION.turn_detection,
        temperature: this.config.SESSION.temperature,
        max_response_output_tokens: 512 // Reduced from 4096 to save costs
      }
    };
    
    console.log('üì§ Sending session config with voice:', this.voice);
    this.sendMessage(config);
  }
  
  // Trigger initial greeting
  triggerGreeting() {
    console.log('üëã Triggering Voxy greeting...');
    
    // Send a message to trigger the AI to speak first
    const greetingTrigger = {
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [
          {
            type: 'input_text',
            text: '[Start conversation - introduce yourself as Voxy]'
          }
        ]
      }
    };
    
    this.sendMessage(greetingTrigger);
    
    // Request response
    const responseRequest = {
      type: 'response.create'
    };
    
    this.sendMessage(responseRequest);
  }
  
  // Handle incoming messages
  handleMessage(event) {
    try {
      const msg = JSON.parse(event.data);
      
      switch (msg.type) {
        case 'session.created':
        case 'session.updated':
          console.log('‚úÖ Session configured');
          break;
          
        case 'conversation.item.input_audio_transcription.completed':
          if (msg.transcript) {
            console.log('üë§ User said:', msg.transcript);
            window.dispatchEvent(new CustomEvent('voxy-user-transcript', { 
              detail: { text: msg.transcript }
            }));
          }
          break;
          
        case 'response.audio_transcript.done':
          if (msg.transcript) {
            console.log('üíú Voxy said:', msg.transcript);
            window.dispatchEvent(new CustomEvent('voxy-assistant-transcript', { 
              detail: { text: msg.transcript }
            }));
          }
          break;
          
        case 'response.audio.delta':
          // Start conversation timer on first AI audio response
          if (!this.conversationStartTime) {
            this.conversationStartTime = Date.now();
            this.startConversationTimer();
            console.log('‚è∞ Conversation timer started - 1 minute limit');
          }
          this.playAudio(msg.delta);
          break;
          
        case 'response.audio.done':
          console.log('üîä Audio response complete');
          window.dispatchEvent(new Event('voxy-response-complete'));
          break;
          
        case 'input_audio_buffer.speech_started':
          console.log('üé§ User started speaking');
          this.stopCurrentAudio();
          window.dispatchEvent(new Event('voxy-user-speaking'));
          break;
          
        case 'input_audio_buffer.speech_stopped':
          console.log('üé§ User stopped speaking');
          window.dispatchEvent(new Event('voxy-user-stopped'));
          break;
          
        case 'error':
          console.error('‚ùå API error:', msg.error);
          break;
      }
    } catch (error) {
      console.error('‚ùå Message handling error:', error);
    }
  }
  
  // Start listening to microphone
  async startListening() {
    console.log('üé§ Starting audio capture...');
    
    // Check if mediaDevices is available
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('MediaDevices API not supported. Please use HTTPS.');
    }
    
    // Check if running on mobile/Android
    const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    
    // Force user interaction on mobile (required for Android)
    if (isMobile && this.audioContext && this.audioContext.state === 'suspended') {
      console.log('üì± Resuming audio context for mobile...');
      await this.audioContext.resume();
    }
    
    // Check current permissions
    try {
      const permissionStatus = await navigator.permissions.query({ name: 'microphone' });
      console.log('üé§ Microphone permission status:', permissionStatus.state);
      
      if (permissionStatus.state === 'denied') {
        throw new Error('Microphone access denied. Please allow microphone access in browser settings.');
      }
    } catch (permError) {
      console.warn('‚ö†Ô∏è Could not check microphone permissions:', permError.message);
    }
    console.log('üì± Mobile device detected:', isMobile);
    
    try {
      // Simplified audio constraints for better mobile compatibility
      const audioConstraints = isMobile ? {
        audio: true // Use default settings on mobile for better compatibility
      } : {
        audio: {
          channelCount: 1,
          sampleRate: 24000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      };
      
      console.log('üé§ Requesting microphone with constraints:', audioConstraints);
      console.log('üé§ About to call getUserMedia...');
      
      this.mediaStream = await navigator.mediaDevices.getUserMedia(audioConstraints);
      console.log('‚úÖ getUserMedia successful, stream:', this.mediaStream);
      
      // Create audio context with mobile-optimized settings
      this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: isMobile ? 44100 : 24000, // Use 44.1kHz on mobile for better compatibility
        latencyHint: 'interactive'
      });
      
      console.log('üé§ Audio context created:', this.audioContext.sampleRate + 'Hz');
      
      // Resume audio context if suspended (required on mobile)
      if (this.audioContext.state === 'suspended') {
        console.log('üé§ Resuming suspended audio context...');
        await this.audioContext.resume();
      }
      
      const source = this.audioContext.createMediaStreamSource(this.mediaStream);
      
      // Use different audio processing approaches for mobile vs desktop
      if (isMobile) {
        console.log('üì± Using mobile-optimized audio processing');
        await this.setupMobileAudioProcessing(source);
      } else {
        console.log('üíª Using desktop audio processing');
        this.setupDesktopAudioProcessing(source);
      }
      
      this.isListening = true;
      console.log('‚úÖ Audio capture started');
      
    } catch (error) {
      console.error('‚ùå Failed to start audio capture:', error);
      console.error('‚ùå Error name:', error.name);
      console.error('‚ùå Error message:', error.message);
      
      // Provide user-friendly error messages
      let userMessage = 'Eroare la accesarea microfonului';
      
      if (error.name === 'NotAllowedError') {
        userMessage = 'Accesul la microfon a fost refuzat. VƒÉ rugƒÉm sƒÉ permite»õi accesul la microfon √Æn setƒÉrile browser-ului.';
      } else if (error.name === 'NotFoundError') {
        userMessage = 'Nu s-a gƒÉsit microfon. VƒÉ rugƒÉm sƒÉ verifica»õi cƒÉ ave»õi un microfon conectat.';
      } else if (error.name === 'NotSupportedError') {
        userMessage = 'Browser-ul nu suportƒÉ accesul la microfon. VƒÉ rugƒÉm sƒÉ folosi»õi HTTPS.';
      } else if (error.name === 'SecurityError') {
        userMessage = 'Eroare de securitate. VƒÉ rugƒÉm sƒÉ folosi»õi HTTPS »ôi sƒÉ permite»õi accesul la microfon.';
      }
      
      throw new Error(userMessage);
    }
  }
  
  // Mobile-optimized audio processing
  async setupMobileAudioProcessing(source) {
    try {
      // Try to use AudioWorklet if available (modern browsers)
      if (this.audioContext.audioWorklet) {
        console.log('üì± Using AudioWorklet for mobile');
        // For now, fallback to ScriptProcessor for compatibility
        this.setupDesktopAudioProcessing(source);
        return;
      }
    } catch (error) {
      console.warn('üì± AudioWorklet not supported, using fallback:', error.message);
    }
    
    // Fallback: Use ScriptProcessor with mobile-optimized settings
    console.log('üì± Using ScriptProcessor fallback for mobile');
    this.setupDesktopAudioProcessing(source);
  }
  
  // Desktop audio processing (ScriptProcessor)
  setupDesktopAudioProcessing(source) {
    // Use smaller buffer size for mobile, larger for desktop
    const bufferSize = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ? 2048 : 4096;
    
    this.audioProcessor = this.audioContext.createScriptProcessor(bufferSize, 1, 1);
    
    this.audioProcessor.onaudioprocess = (event) => {
      if (!this.isConnected || !this.isListening || this.isProcessingAudio) return;
      
      // Rate limiting: Only process audio chunks every 100ms
      const now = Date.now();
      if (now - this.lastAudioChunkTime < this.audioChunkInterval) {
        return;
      }
      
      this.isProcessingAudio = true;
      this.lastAudioChunkTime = now;
      
      const inputData = event.inputBuffer.getChannelData(0);
      const pcm16 = new Int16Array(inputData.length);
      
      for (let i = 0; i < inputData.length; i++) {
        const s = Math.max(-1, Math.min(1, inputData[i]));
        pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      
      const base64Audio = this.arrayBufferToBase64(pcm16.buffer);
      this.sendAudioChunk(base64Audio);
      
      // Reset processing flag after a short delay
      setTimeout(() => {
        this.isProcessingAudio = false;
      }, 50);
    };
    
    source.connect(this.audioProcessor);
    this.audioProcessor.connect(this.audioContext.destination);
  }

  // Stop listening
  stopListening() {
    console.log('üõë Stopping audio capture...');
    
    if (this.audioProcessor) {
      this.audioProcessor.disconnect();
      this.audioProcessor = null;
    }
    
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
      this.mediaStream = null;
    }
    
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    
    this.isListening = false;
    this.nextPlayTime = 0;
    console.log('‚úÖ Audio capture stopped');
  }
  
  // Send audio chunk to API with additional logging
  sendAudioChunk(base64Audio) {
    const message = {
      type: 'input_audio_buffer.append',
      audio: base64Audio
    };
    
    console.log('üé§ Sending audio chunk (rate limited)');
    this.sendMessage(message);
  }
  
  // Play audio from API
  playAudio(base64Audio) {
    if (!this.audioContext) {
      this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 24000
      });
      this.nextPlayTime = this.audioContext.currentTime;
    }
    
    // Decode base64 to ArrayBuffer
    const binaryString = atob(base64Audio);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    
    // Convert PCM16 to Float32
    const pcm16 = new Int16Array(bytes.buffer);
    const float32 = new Float32Array(pcm16.length);
    for (let i = 0; i < pcm16.length; i++) {
      float32[i] = pcm16[i] / (pcm16[i] < 0 ? 0x8000 : 0x7FFF);
    }
    
    // Create audio buffer
    const audioBuffer = this.audioContext.createBuffer(1, float32.length, 24000);
    audioBuffer.copyToChannel(float32, 0);
    
    // Create buffer source
    const source = this.audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.playbackRate.value = this.config.AUDIO.playbackSpeed;
    source.connect(this.audioContext.destination);
    
    // Calculate timing to avoid overlap
    const currentTime = this.audioContext.currentTime;
    const startTime = Math.max(currentTime, this.nextPlayTime);
    const chunkDuration = audioBuffer.duration / source.playbackRate.value;
    this.nextPlayTime = startTime + chunkDuration;
    
    // Track audio source
    this.currentAudioSources.push(source);
    source.onended = () => {
      const index = this.currentAudioSources.indexOf(source);
      if (index > -1) {
        this.currentAudioSources.splice(index, 1);
      }
    };
    
    // Play audio
    source.start(startTime);
  }
  
  // Stop currently playing audio
  stopCurrentAudio() {
    this.currentAudioSources.forEach(source => {
      try {
        source.stop();
      } catch (e) {
        // Source might have already stopped
      }
    });
    this.currentAudioSources = [];
    
    if (this.audioContext) {
      this.nextPlayTime = this.audioContext.currentTime;
    }
  }
  
  // Start conversation timer
  startConversationTimer() {
    this.timeLimitTimer = setTimeout(() => {
      console.log('‚è∞ Conversation time limit reached (1 minute)');
      this.endConversationDueToTimeLimit();
    }, this.conversationTimeLimit);
  }
  
  // End conversation due to time limit
  endConversationDueToTimeLimit() {
    console.log('‚è∞ Ending conversation due to 1-minute time limit');
    
    // Stop listening and disconnect
    this.stopListening();
    this.disconnect();
    
    // Dispatch event to update UI
    window.dispatchEvent(new CustomEvent('voxy-time-limit-reached', {
      detail: { message: 'Conversa»õia s-a √Æncheiat dupƒÉ 1 minut de testare' }
    }));
  }
  
  // Clear conversation timer
  clearConversationTimer() {
    if (this.timeLimitTimer) {
      clearTimeout(this.timeLimitTimer);
      this.timeLimitTimer = null;
    }
    this.conversationStartTime = null;
  }

  // Disconnect and cleanup
  disconnect() {
    console.log('üîå Disconnecting...');
    
    this.stopListening();
    this.stopCurrentAudio();
    this.clearConversationTimer();
    
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
    
    this.isConnected = false;
    console.log('‚úÖ Disconnected');
  }
  
  // Helper: Convert ArrayBuffer to Base64
  arrayBufferToBase64(buffer) {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }
  
  // Helper: Send WebSocket message
  sendMessage(message) {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify(message));
    }
  }
}

// Export for use in other scripts
if (typeof window !== 'undefined') {
  window.VoxyVoice = VoxyVoice;
}

