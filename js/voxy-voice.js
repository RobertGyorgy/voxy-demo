/* ================================
   VOXY VOICE AI INTEGRATION
   ================================
   Integrates OpenAI Realtime API for voice conversations
   Based on realtime-voice-ai backend
*/

class VoxyVoice {
  constructor(apiKey, voice = 'ballad') {
    console.log('ðŸŽ¤ VoxyVoice constructor called with voice:', voice);
    
    this.apiKey = apiKey;
    this.voice = voice;
    this.ws = null;
    this.isConnected = false;
    this.isListening = false;
    
    // Audio contexts
    this.audioContext = null;
    this.mediaStream = null;
    this.audioProcessor = null;
    this.nextPlayTime = 0;
    this.currentAudioSources = [];
    
    // Configuration from realtime-voice-ai/config.js
    this.config = {
      REALTIME_API_URL: 'wss://api.openai.com/v1/realtime',
      MODEL: 'gpt-realtime-mini',
      AUDIO: {
        sampleRate: 24000,
        playbackSpeed: 0.95
      },
      SESSION: {
        turn_detection: {
          type: 'server_vad',
          threshold: 0.5,
          prefix_padding_ms: 300,
          silence_duration_ms: 1500
        },
        temperature: 0.8,
        instructions: `Tu eÈ™ti Voxy â€” un agent vocal conversaÈ›ional de ultimÄƒ generaÈ›ie, dezvoltat manual de o echipÄƒ ambiÈ›ioasÄƒ din BraÈ™ov, RomÃ¢nia.
EÈ™ti creatÄƒ pentru a vorbi natural, fluid È™i inteligent, Ã®n peste 40 de limbi.

PRIMA INTERACÈšIUNE - SALUTUL TÄ‚U INIÈšIAL:
CÃ¢nd eÈ™ti activatÄƒ pentru prima datÄƒ, te prezinÈ›i astfel:
"Salut! Eu sunt Voxy, agentul vocal AI creat de o echipÄƒ din BraÈ™ov. Pot vorbi Ã®n peste 40 de limbi È™i ajut companiile sÄƒ Ã®nlocuiascÄƒ operatorii de call center. Cu ce te pot ajuta astÄƒzi?"

DupÄƒ salutul iniÈ›ial, conduci conversaÈ›ia natural:
- AsculÈ›i activ È™i rÄƒspunzi relevant
- Pui Ã®ntrebÄƒri clare despre nevoile utilizatorului
- Explici funcÈ›ionalitÄƒÈ›ile VOXY cÃ¢nd eÈ™ti Ã®ntrebatÄƒ
- MenÈ›ii un ton profesionist dar prietenos
- RÄƒspunzi concis È™i clar

FUNCÈšIONALITÄ‚ÈšI VOXY (pentru cÃ¢nd eÈ™ti Ã®ntrebatÄƒ):
1. RÄƒspunde la apeluri telefonice 24/7
2. Face apeluri de vÃ¢nzÄƒri automate
3. Se integreazÄƒ cu CRM-uri
4. VorbeÈ™te Ã®n 40+ limbi
5. AnalizÄƒ È™i raportare automatÄƒ

VorbeÈ™te natural, fÄƒrÄƒ jargon tehnic, È™i adapteazÄƒ-te la Ã®ntrebÄƒrile utilizatorului.`
      }
    };
  }
  
  // Connect to OpenAI Realtime API
  async connect() {
    console.log('ðŸ”Œ Connecting to OpenAI Realtime API...');
    
    // Load API key if not already set
    if (!this.apiKey) {
      loadVoxyApiKey();
      if (!VOXY_CONFIG.apiKey) {
        throw new Error('No API key available. Please provide OpenAI API key.');
      }
      this.apiKey = VOXY_CONFIG.apiKey;
    }
    
    console.log('ðŸ”‘ Using API key:', this.apiKey.substring(0, 20) + '...');
    
    return new Promise((resolve, reject) => {
      const url = `${this.config.REALTIME_API_URL}?model=${this.config.MODEL}`;
      console.log('ðŸŒ WebSocket URL:', url);
      
      this.ws = new WebSocket(url, [
        'realtime',
        `openai-insecure-api-key.${this.apiKey}`,
        'openai-beta.realtime-v1'
      ]);
      
      this.ws.binaryType = 'arraybuffer';
      
      this.ws.onopen = () => {
        console.log('âœ… WebSocket connected successfully');
        this.isConnected = true;
        this.sendSessionConfig();
        resolve();
      };
      
      this.ws.onmessage = (event) => {
        console.log('ðŸ“¨ WebSocket message received:', event.data);
        this.handleMessage(event);
      };
      
      this.ws.onerror = (error) => {
        console.error('âŒ WebSocket error:', error);
        console.error('âŒ Error details:', error.type, error.code);
        console.error('âŒ WebSocket readyState:', this.ws.readyState);
        reject(error);
      };
      
      this.ws.onclose = (event) => {
        console.log('ðŸ”Œ WebSocket closed:', event.code, event.reason);
        this.isConnected = false;
        if (this.isListening) {
          this.stopListening();
        }
      };
      
      // Timeout after 5 seconds
      const timeoutId = setTimeout(() => {
        if (!this.isConnected) {
          console.error('â° Connection timeout after 5 seconds');
          console.error('â° WebSocket readyState:', this.ws.readyState);
          reject(new Error('Connection timeout after 5 seconds'));
        }
      }, 5000);
      
      // Clear timeout if connection succeeds
      this.ws.addEventListener('open', () => {
        clearTimeout(timeoutId);
      });
    });
  }
  
  // Send session configuration
  sendSessionConfig() {
    const config = {
      type: 'session.update',
      session: {
        modalities: ['text', 'audio'],
        instructions: this.config.SESSION.instructions,
        voice: this.voice,
        input_audio_format: 'pcm16',
        output_audio_format: 'pcm16',
        input_audio_transcription: {
          model: 'whisper-1'
        },
        turn_detection: this.config.SESSION.turn_detection,
        temperature: this.config.SESSION.temperature,
        max_response_output_tokens: 4096
      }
    };
    
    console.log('ðŸ“¤ Sending session config with voice:', this.voice);
    this.sendMessage(config);
  }
  
  // Trigger initial greeting
  triggerGreeting() {
    console.log('ðŸ‘‹ Triggering Voxy greeting...');
    
    // Send a message to trigger the AI to speak first
    const greetingTrigger = {
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [
          {
            type: 'input_text',
            text: '[Start conversation - introduce yourself as Voxy]'
          }
        ]
      }
    };
    
    this.sendMessage(greetingTrigger);
    
    // Request response
    const responseRequest = {
      type: 'response.create'
    };
    
    this.sendMessage(responseRequest);
  }
  
  // Handle incoming messages
  handleMessage(event) {
    try {
      const msg = JSON.parse(event.data);
      
      switch (msg.type) {
        case 'session.created':
        case 'session.updated':
          console.log('âœ… Session configured');
          break;
          
        case 'conversation.item.input_audio_transcription.completed':
          if (msg.transcript) {
            console.log('ðŸ‘¤ User said:', msg.transcript);
            window.dispatchEvent(new CustomEvent('voxy-user-transcript', { 
              detail: { text: msg.transcript }
            }));
          }
          break;
          
        case 'response.audio_transcript.done':
          if (msg.transcript) {
            console.log('ðŸ’œ Voxy said:', msg.transcript);
            window.dispatchEvent(new CustomEvent('voxy-assistant-transcript', { 
              detail: { text: msg.transcript }
            }));
          }
          break;
          
        case 'response.audio.delta':
          this.playAudio(msg.delta);
          break;
          
        case 'response.audio.done':
          console.log('ðŸ”Š Audio response complete');
          window.dispatchEvent(new Event('voxy-response-complete'));
          break;
          
        case 'input_audio_buffer.speech_started':
          console.log('ðŸŽ¤ User started speaking');
          this.stopCurrentAudio();
          window.dispatchEvent(new Event('voxy-user-speaking'));
          break;
          
        case 'input_audio_buffer.speech_stopped':
          console.log('ðŸŽ¤ User stopped speaking');
          window.dispatchEvent(new Event('voxy-user-stopped'));
          break;
          
        case 'error':
          console.error('âŒ API error:', msg.error);
          break;
      }
    } catch (error) {
      console.error('âŒ Message handling error:', error);
    }
  }
  
  // Start listening to microphone
  async startListening() {
    console.log('ðŸŽ¤ Starting audio capture...');
    
    // Check if mediaDevices is available
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('MediaDevices API not supported. Please use HTTPS.');
    }
    
    // Check if running on mobile/Android
    const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    
    // Force user interaction on mobile (required for Android)
    if (isMobile && this.audioContext && this.audioContext.state === 'suspended') {
      console.log('ðŸ“± Resuming audio context for mobile...');
      await this.audioContext.resume();
    }
    
    // Check current permissions
    try {
      const permissionStatus = await navigator.permissions.query({ name: 'microphone' });
      console.log('ðŸŽ¤ Microphone permission status:', permissionStatus.state);
      
      if (permissionStatus.state === 'denied') {
        throw new Error('Microphone access denied. Please allow microphone access in browser settings.');
      }
    } catch (permError) {
      console.warn('âš ï¸ Could not check microphone permissions:', permError.message);
    }
    console.log('ðŸ“± Mobile device detected:', isMobile);
    
    try {
      // Simplified audio constraints for better mobile compatibility
      const audioConstraints = isMobile ? {
        audio: true // Use default settings on mobile for better compatibility
      } : {
        audio: {
          channelCount: 1,
          sampleRate: 24000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      };
      
      console.log('ðŸŽ¤ Requesting microphone with constraints:', audioConstraints);
      console.log('ðŸŽ¤ About to call getUserMedia...');
      
      this.mediaStream = await navigator.mediaDevices.getUserMedia(audioConstraints);
      console.log('âœ… getUserMedia successful, stream:', this.mediaStream);
      
      // Create audio context with mobile-optimized settings
      this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: isMobile ? 44100 : 24000, // Use 44.1kHz on mobile for better compatibility
        latencyHint: 'interactive'
      });
      
      console.log('ðŸŽ¤ Audio context created:', this.audioContext.sampleRate + 'Hz');
      
      // Resume audio context if suspended (required on mobile)
      if (this.audioContext.state === 'suspended') {
        console.log('ðŸŽ¤ Resuming suspended audio context...');
        await this.audioContext.resume();
      }
      
      const source = this.audioContext.createMediaStreamSource(this.mediaStream);
      
      // Use different audio processing approaches for mobile vs desktop
      if (isMobile) {
        console.log('ðŸ“± Using mobile-optimized audio processing');
        await this.setupMobileAudioProcessing(source);
      } else {
        console.log('ðŸ’» Using desktop audio processing');
        this.setupDesktopAudioProcessing(source);
      }
      
      this.isListening = true;
      console.log('âœ… Audio capture started');
      
    } catch (error) {
      console.error('âŒ Failed to start audio capture:', error);
      console.error('âŒ Error name:', error.name);
      console.error('âŒ Error message:', error.message);
      
      // Provide user-friendly error messages
      let userMessage = 'Eroare la accesarea microfonului';
      
      if (error.name === 'NotAllowedError') {
        userMessage = 'Accesul la microfon a fost refuzat. VÄƒ rugÄƒm sÄƒ permiteÈ›i accesul la microfon Ã®n setÄƒrile browser-ului.';
      } else if (error.name === 'NotFoundError') {
        userMessage = 'Nu s-a gÄƒsit microfon. VÄƒ rugÄƒm sÄƒ verificaÈ›i cÄƒ aveÈ›i un microfon conectat.';
      } else if (error.name === 'NotSupportedError') {
        userMessage = 'Browser-ul nu suportÄƒ accesul la microfon. VÄƒ rugÄƒm sÄƒ folosiÈ›i HTTPS.';
      } else if (error.name === 'SecurityError') {
        userMessage = 'Eroare de securitate. VÄƒ rugÄƒm sÄƒ folosiÈ›i HTTPS È™i sÄƒ permiteÈ›i accesul la microfon.';
      }
      
      throw new Error(userMessage);
    }
  }
  
  // Mobile-optimized audio processing
  async setupMobileAudioProcessing(source) {
    try {
      // Try to use AudioWorklet if available (modern browsers)
      if (this.audioContext.audioWorklet) {
        console.log('ðŸ“± Using AudioWorklet for mobile');
        // For now, fallback to ScriptProcessor for compatibility
        this.setupDesktopAudioProcessing(source);
        return;
      }
    } catch (error) {
      console.warn('ðŸ“± AudioWorklet not supported, using fallback:', error.message);
    }
    
    // Fallback: Use ScriptProcessor with mobile-optimized settings
    console.log('ðŸ“± Using ScriptProcessor fallback for mobile');
    this.setupDesktopAudioProcessing(source);
  }
  
  // Desktop audio processing (ScriptProcessor)
  setupDesktopAudioProcessing(source) {
    // Use smaller buffer size for mobile, larger for desktop
    const bufferSize = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ? 2048 : 4096;
    
    this.audioProcessor = this.audioContext.createScriptProcessor(bufferSize, 1, 1);
    
    this.audioProcessor.onaudioprocess = (event) => {
      if (!this.isConnected || !this.isListening) return;
      
      const inputData = event.inputBuffer.getChannelData(0);
      const pcm16 = new Int16Array(inputData.length);
      
      for (let i = 0; i < inputData.length; i++) {
        const s = Math.max(-1, Math.min(1, inputData[i]));
        pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      
      const base64Audio = this.arrayBufferToBase64(pcm16.buffer);
      this.sendAudioChunk(base64Audio);
    };
    
    source.connect(this.audioProcessor);
    this.audioProcessor.connect(this.audioContext.destination);
  }

  // Stop listening
  stopListening() {
    console.log('ðŸ›‘ Stopping audio capture...');
    
    if (this.audioProcessor) {
      this.audioProcessor.disconnect();
      this.audioProcessor = null;
    }
    
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
      this.mediaStream = null;
    }
    
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    
    this.isListening = false;
    this.nextPlayTime = 0;
    console.log('âœ… Audio capture stopped');
  }
  
  // Send audio chunk to API
  sendAudioChunk(base64Audio) {
    const message = {
      type: 'input_audio_buffer.append',
      audio: base64Audio
    };
    
    this.sendMessage(message);
  }
  
  // Play audio from API
  playAudio(base64Audio) {
    if (!this.audioContext) {
      this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 24000
      });
      this.nextPlayTime = this.audioContext.currentTime;
    }
    
    // Decode base64 to ArrayBuffer
    const binaryString = atob(base64Audio);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    
    // Convert PCM16 to Float32
    const pcm16 = new Int16Array(bytes.buffer);
    const float32 = new Float32Array(pcm16.length);
    for (let i = 0; i < pcm16.length; i++) {
      float32[i] = pcm16[i] / (pcm16[i] < 0 ? 0x8000 : 0x7FFF);
    }
    
    // Create audio buffer
    const audioBuffer = this.audioContext.createBuffer(1, float32.length, 24000);
    audioBuffer.copyToChannel(float32, 0);
    
    // Create buffer source
    const source = this.audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.playbackRate.value = this.config.AUDIO.playbackSpeed;
    source.connect(this.audioContext.destination);
    
    // Calculate timing to avoid overlap
    const currentTime = this.audioContext.currentTime;
    const startTime = Math.max(currentTime, this.nextPlayTime);
    const chunkDuration = audioBuffer.duration / source.playbackRate.value;
    this.nextPlayTime = startTime + chunkDuration;
    
    // Track audio source
    this.currentAudioSources.push(source);
    source.onended = () => {
      const index = this.currentAudioSources.indexOf(source);
      if (index > -1) {
        this.currentAudioSources.splice(index, 1);
      }
    };
    
    // Play audio
    source.start(startTime);
  }
  
  // Stop currently playing audio
  stopCurrentAudio() {
    this.currentAudioSources.forEach(source => {
      try {
        source.stop();
      } catch (e) {
        // Source might have already stopped
      }
    });
    this.currentAudioSources = [];
    
    if (this.audioContext) {
      this.nextPlayTime = this.audioContext.currentTime;
    }
  }
  
  // Disconnect and cleanup
  disconnect() {
    console.log('ðŸ”Œ Disconnecting...');
    
    this.stopListening();
    this.stopCurrentAudio();
    
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
    
    this.isConnected = false;
    console.log('âœ… Disconnected');
  }
  
  // Helper: Convert ArrayBuffer to Base64
  arrayBufferToBase64(buffer) {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }
  
  // Helper: Send WebSocket message
  sendMessage(message) {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify(message));
    }
  }
}

// Export for use in other scripts
if (typeof window !== 'undefined') {
  window.VoxyVoice = VoxyVoice;
}

